Road Follower - Train Model
In this notebook we will train a neural network to take an input image, and output a set of x, y values corresponding to a target.

We will be using PyTorch deep learning framework to train ResNet18 neural network architecture model for road follower application.

import torch
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.datasets as datasets
import torchvision.models as models
import torchvision.transforms as transforms
import glob
import PIL.Image
import os
import numpy as np
Download and extract data
Before you start, you should upload the road_following_<Date&Time>.zip file that you created in the data_collection.ipynb notebook on the robot.

If you're training on the JetBot you collected data on, you can skip this!

You should then extract this dataset by calling the command below:

!unzip -q road_following.zip
unzip:  cannot find or open road_following.zip, road_following.zip.zip or road_following.zip.ZIP.
You should see a folder named dataset_all appear in the file browser.

Create Dataset Instance
Here we create a custom torch.utils.data.Dataset implementation, which implements the __len__ and __getitem__ functions. This class is responsible for loading images and parsing the x, y values from the image filenames. Because we implement the torch.utils.data.Dataset class, we can use all of the torch data utilities :)

We hard coded some transformations (like color jitter) into our dataset. We made random horizontal flips optional (in case you want to follow a non-symmetric path, like a road where we need to 'stay right'). If it doesn't matter whether your robot follows some convention, you could enable flips to augment the dataset.

def get_x(path, width):
    """Gets the x value from the image filename"""
    return (float(int(path.split("_")[1])) - width/2) / (width/2)
​
def get_y(path, height):
    """Gets the y value from the image filename"""
    return (float(int(path.split("_")[2])) - height/2) / (height/2)
​
class XYDataset(torch.utils.data.Dataset):
    
    def __init__(self, directory, random_hflips=False):
        self.directory = directory
        self.random_hflips = random_hflips
        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))
        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        
        image = PIL.Image.open(image_path)
        width, height = image.size
        x = float(get_x(os.path.basename(image_path), width))
        y = float(get_y(os.path.basename(image_path), height))
      
        if float(np.random.rand(1)) > 0.5:
            image = transforms.functional.hflip(image)
            x = -x
        
        image = self.color_jitter(image)
        image = transforms.functional.resize(image, (224, 224))
        image = transforms.functional.to_tensor(image)
        image = image.numpy()[::-1].copy()
        image = torch.from_numpy(image)
        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        
        return image, torch.tensor([x, y]).float()
    
dataset = XYDataset('dataset_xy', random_hflips=False)
Split dataset into train and test sets
Once we read dataset, we will split data set in train and test sets. In this example we split train and test a 90%-10%. The test set will be used to verify the accuracy of the model we train.

test_percent = 0.1
num_test = int(test_percent * len(dataset))
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])
Create data loaders to load data in batches
We use DataLoader class to load data in batches, shuffle data and allow using multi-subprocesses. In this example we use batch size of 64. Batch size will be based on memory available with your GPU and it can impact accuracy of the model.

train_loader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=8,
    shuffle=True,
    num_workers=0
)
​
test_loader = torch.utils.data.DataLoader(
    test_dataset,
    batch_size=8,
    shuffle=True,
    num_workers=0
)
Define Neural Network Model
We use ResNet-18 model available on PyTorch TorchVision.

In a process called transfer learning, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.

More details on ResNet-18 : https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py

More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE

model = models.resnet18(pretrained=True)
ResNet model has fully connect (fc) final layer with 512 as in_features and we will be training for regression thus out_features as 1

Finally, we transfer our model for execution on the GPU

model.fc = torch.nn.Linear(512, 2)
device = torch.device('cuda')
model = model.to(device)
Train Regression:
We train for 50 epochs and save best model if the loss is reduced.

NUM_EPOCHS = 70
BEST_MODEL_PATH = 'best_steering_model_xy.pth'
best_loss = 1e9
​
optimizer = optim.Adam(model.parameters())
​
for epoch in range(NUM_EPOCHS):
    
    model.train()
    train_loss = 0.0
    for images, labels in iter(train_loader):
        images = images.to(device)
        labels = labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = F.mse_loss(outputs, labels)
        train_loss += float(loss)
        loss.backward()
        optimizer.step()
    train_loss /= len(train_loader)
    
    model.eval()
    test_loss = 0.0
    for images, labels in iter(test_loader):
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        loss = F.mse_loss(outputs, labels)
        test_loss += float(loss)
    test_loss /= len(test_loader)
    
    print('%f, %f' % (train_loss, test_loss))
    if test_loss < best_loss:
        torch.save(model.state_dict(), BEST_MODEL_PATH)
        best_loss = test_loss
0.273165, 0.070438
0.068529, 0.040855
0.047399, 0.046945
0.054715, 0.050958
0.036108, 0.027775
0.033000, 0.035386
0.028137, 0.026799
0.035124, 0.021381
0.021004, 0.013833
0.024429, 0.033786
0.018805, 0.022145
0.018074, 0.022468
0.013296, 0.017078
0.015082, 0.011488
0.012647, 0.011274
0.011780, 0.014073
0.010114, 0.012985
0.011264, 0.021601
0.010311, 0.018188
0.007399, 0.015696
0.011233, 0.015482
0.009311, 0.010343
0.008329, 0.014227
0.007883, 0.010616
0.007943, 0.011142
0.007148, 0.014638
0.010219, 0.017872
0.008637, 0.008989
0.007040, 0.014902
0.007859, 0.010547
0.004609, 0.009990
0.006952, 0.009293
0.008189, 0.016650
0.005769, 0.008687
0.006644, 0.012945
0.007734, 0.008690
0.004764, 0.008939
0.004569, 0.012227
0.004086, 0.010788
0.005210, 0.008895
0.004047, 0.019303
0.004782, 0.008888
0.004315, 0.008657
0.003533, 0.013405
0.005268, 0.011782
0.005194, 0.011989
0.005026, 0.010299
0.004932, 0.009869
0.004809, 0.012093
0.006016, 0.013433
0.004911, 0.009427
0.005021, 0.010743
0.004621, 0.008861
0.003773, 0.008851
0.004048, 0.013284
0.003344, 0.008235
0.003934, 0.010991
0.005100, 0.011953
0.004366, 0.013624
0.004323, 0.009413
0.006009, 0.016564
0.006359, 0.018762
0.004958, 0.027329
0.006989, 0.019119
0.008365, 0.012630
0.009755, 0.013947
0.032340, 0.022538
0.022402, 0.022605
0.013816, 0.012121
0.007647, 0.010200
Once the model is trained, it will generate best_steering_model_xy.pth file which you can use for inferencing in the live demo notebook.

If you trained on a different machine other than JetBot, you'll need to upload this to the JetBot to the road_following example folder.

